# Finance TechStack - SEC Filings & Financial Data Aggregation

A production-ready Python application that integrates SEC EDGAR filings, XBRL fundamentals, and Alpha Vantage financial data using Prefect workflow orchestration. Efficiently process and store financial data in Apache Parquet format.

## Features

- **Multi-source financial data aggregation**: Combines SEC EDGAR, XBRL fundamentals, and Alpha Vantage data
- **SEC EDGAR integration**: Automatic CIK lookup and filing retrieval for all SEC filing types
- **XBRL parsing**: Extracts financial metrics from SEC submissions (10-K, 10-Q, etc.)
- **Alpha Vantage integration**: Fetch fundamental and technical data (when API key configured)
- **Prefect workflows**: Production-grade task orchestration with automatic retries and error handling
- **Efficient data storage**: Apache Parquet format with Snappy compression
- **Comprehensive testing**: 33+ unit and integration tests with Prefect support
- **Rate limiting**: Built-in delays to respect SEC API rate limits
- **Advanced logging**: Prefect integration with detailed task-level logging
- **Docker containerization**: Full multi-stage build, docker-compose orchestration, cloud-ready
- **Scalable architecture**: Ready for Dask/Coiled for distributed batch processing
- **Multi-cloud deployment**: AWS ECS, Kubernetes, and other cloud platforms supported

## Quick Start

### Prerequisites

- Python 3.13 or higher
- `uv` package manager (or `pip` + `venv`)

### Installation

```bash
# Clone the repository
cd /Users/conordonohue/Desktop/TechStack

# Install dependencies with uv
uv sync

# Or with pip
python -m venv .venv
source .venv/bin/activate
pip install -e .
```

### Run the Main Flow

```bash
# Run financial data aggregation for AAPL and MSFT
uv run python src/main.py
```

This will:
1. Fetch CIK identifiers for the specified tickers
2. Retrieve the latest SEC 10-K filing metadata
3. Extract XBRL financial fundamentals
4. Fetch Alpha Vantage data (if API key is configured)
5. Merge all data sources into a unified record
6. Save aggregated data to `db/financial_data_<timestamp>.parquet`

## Usage Examples

### Run Prefect Flow Directly

```python
from src.main import aggregate_financial_data

# Aggregate financial data for multiple companies
result = aggregate_financial_data(
    tickers=["AAPL", "MSFT", "GOOGL"],
    filing_type="10-K",
    include_alpha_vantage=True
)

print(f"Processed: {result['total_tickers']} tickers")
print(f"Successful: {result['successful']}")
print(f"Output file: {result['output_file']}")
```

### Fetch Individual Data Sources

```python
from src.main import fetch_company_cik, fetch_filing_metadata, fetch_xbrl_data
from src.alpha_vantage import fetch_fundamental_data

# Get company info and CIK
company = fetch_company_cik("AAPL")
cik = company["cik"]

# Get latest 10-K filing
filing = fetch_filing_metadata(cik, "10-K")

# Get XBRL fundamentals
xbrl = fetch_xbrl_data(cik, "AAPL")

# Get Alpha Vantage data
av_data = fetch_fundamental_data("AAPL")
```

## Testing

The project includes 33 passing tests covering all components:

### Run Unit Tests

```bash
# Run all tests
uv run --with pytest python -m pytest tests/ -v

# Run specific test file
uv run --with pytest python -m pytest tests/test_sec_scraper.py -v

# Run with coverage
uv run --with pytest python -m pytest tests/ --cov=src
```

### Run Tests with Prefect Integration

```bash
# Run comprehensive Prefect test suite
uv run python tests/test_integration.py
```

### Test Results

- ✓ 33 tests passed
- ✓ 9 tests skipped (Prefect task network mocking - see test_sec_scraper.py)
- ✓ CIK lookup validation for multiple tickers
- ✓ SEC filing metadata extraction
- ✓ XBRL financial data parsing
- ✓ Parquet data storage and integrity
- ✓ Error handling and edge cases
- ✓ Integration with all data sources

## Project Structure

```
.
├── src/
│   ├── main.py                 # Main Prefect flow and tasks
│   ├── xbrl.py                 # XBRL data extraction
│   ├── alpha_vantage.py        # Alpha Vantage API integration
│   ├── data_merge.py           # Data aggregation utilities
│   ├── config.py               # Configuration loader
│   ├── constants.py            # Project constants
│   ├── exceptions.py           # Custom exceptions
│   ├── utils.py                # Helper utilities
│   └── __init__.py             # Package exports
├── tests/
│   ├── test_sec_scraper.py     # Unit tests (22 test cases)
│   ├── test_xbrl.py            # XBRL parsing tests (11 passed, 9 skipped)
│   ├── test_integration.py     # Prefect integration tests
│   └── run_tests.py            # Interactive test menu
├── db/                         # Data storage directory
├── config.csv                  # API configuration (create from template)
├── config.csv.template         # Configuration template
├── pyproject.toml              # Project metadata and dependencies
├── README.md                   # This file
├── TESTING.md                  # Testing documentation
└── .github/
    └── copilot-instructions.md # Development guidelines
```

## Configuration

### API Keys

Create `config.csv` from the template:

```bash
cp config.csv.template config.csv
```

Then add your API keys:

```csv
API_KEY,VALUE
ALPHA_VANTAGE_API_KEY,your_key_here
```

### Environment Variables

Alternatively, set environment variables:

```bash
export ALPHA_VANTAGE_API_KEY=your_key_here
```

## Data Sources

### SEC EDGAR
- **Source**: https://www.sec.gov/
- **Rate Limit**: ~10 requests per second
- **Data**: Company filings (10-K, 10-Q, 8-K, etc.)

### XBRL Fundamentals
- **Source**: SEC's company facts JSON API
- **Data**: Structured financial metrics (revenue, income, assets, etc.)
- **Metrics Extracted**:
  - Revenue
  - Net income
  - Total assets/liabilities
  - Shareholders' equity
  - Financial ratios (debt-to-equity, current ratio)

### Alpha Vantage (optional)
- **Source**: https://www.alphavantage.co/
- **Requires**: Free API key
- **Data**: Additional fundamental and technical indicators

## Data Storage

### Parquet Format

Data is saved in Apache Parquet format with Snappy compression:

```python
# Read saved data
import pandas as pd

df = pd.read_parquet("db/financial_data_20251129_143435.parquet")
print(df.head())
print(df.info())

# Export to CSV
df.to_csv("financial_data.csv", index=False)
```

**Benefits**:
- Efficient columnar storage (~90% compression vs CSV)
- Schema validation and type safety
- Fast analytical queries
- Language and platform agnostic

## Prefect Workflow Monitoring

### Start Prefect Server

```bash
uv run python -m prefect server start
```

Then visit http://localhost:4200 to view:
- Flow run history and logs
- Task execution timelines
- Performance metrics
- Error tracking

### View Flow Runs in Terminal

```bash
# List recent flow runs
uv run python -m prefect flow-run ls

# View specific flow run
uv run python -m prefect flow-run inspect <run-id>
```

## Docker & Cloud Deployment

The project is fully containerized and ready for cloud deployment.

### Quick Docker Start

```bash
# Start all services (PostgreSQL, Prefect Server, Worker, App)
docker-compose up -d

# View running services
docker-compose ps

# Run tests in Docker
docker-compose run --rm techstack python -m pytest tests/ -v

# Monitor logs
docker-compose logs -f techstack
```

### Deployment Options

- **Docker Compose**: Local development with all services (PostgreSQL, Prefect, Redis)
- **AWS ECS**: Fargate deployment with Terraform IaC
- **Kubernetes**: Multi-cloud K8s deployment with auto-scaling
- **GCP/Azure/DigitalOcean**: Container-ready deployment

See [DOCKER.md](DOCKER.md) for comprehensive deployment guide.

### Scaling with Dask/Coiled

For batch processing of thousands of companies:

```bash
# Option 1: Local Dask cluster (add to docker-compose.yml)
# Process using distributed workers on your infrastructure

# Option 2: Coiled (managed Dask on AWS)
# Auto-scaling clusters, cost optimization, fully managed
```

See [DOCKER.md - Scaling with Dask and Coiled](DOCKER.md#scaling-with-dask-and-coiled) for details.

## Architecture

### Prefect Tasks

All data fetching operations are implemented as Prefect tasks with:
- **Automatic retries**: 3 retries with 5-second delays
- **Error handling**: Graceful degradation when data sources unavailable
- **Logging**: Detailed Prefect task logs
- **Type validation**: Input/output validation

### Flow Structure

```
aggregate_financial_data (Main Flow)
├── [For each ticker]
│   ├── fetch_company_cik (task)
│   ├── fetch_filing_metadata (task)
│   ├── fetch_xbrl_data (task)
│   │   ├── fetch_xbrl_document (task)
│   │   └── parse_xbrl_fundamentals (task)
│   ├── fetch_alpha_vantage_data (task)
│   └── merge_financial_data (task)
└── save_aggregated_data (task)
```

## Requirements

- **Core**:
  - `prefect>=3.6.4` - Workflow orchestration
  - `requests>=2.31.0` - HTTP client
  - `beautifulsoup4>=4.12.0` - HTML parsing
  - `pandas>=2.0.0` - Data manipulation
  - `pyarrow>=14.0.0` - Parquet support
  - `python-dotenv>=1.0.0` - Environment variables

- **Development**:
  - `pytest>=7.4.0` - Testing framework
  - `pytest-asyncio>=0.21.0` - Async test support

See `pyproject.toml` for complete dependencies.

## Troubleshooting

### Import Errors

If you get import errors, ensure the package is installed in development mode:

```bash
cd /Users/conordonohue/Desktop/TechStack
uv sync
# or
pip install -e .
```

### Network Errors

The scraper retries failed requests up to 3 times. If you continue getting errors:

1. Check your internet connection
2. Verify SEC servers are accessible: https://www.sec.gov/
3. Check your IP hasn't been rate limited (wait 1+ hour)
4. Review Prefect logs: `uv run python -m prefect flow-run inspect <run-id>`

### API Key Issues

For Alpha Vantage integration:
1. Get a free API key from https://www.alphavantage.co/
2. Add it to `config.csv` or set `ALPHA_VANTAGE_API_KEY` environment variable
3. The scraper gracefully skips Alpha Vantage data if the key is not configured

## Performance

### Typical Execution Times

- Single ticker (AAPL): ~18 seconds
- Three tickers (AAPL, MSFT, GOOGL): ~25 seconds
- Includes network delays and SEC API latency

### Optimization Tips

- Cache CIK lookups for repeated tickers
- Run flow on a schedule using Prefect Deployments
- Batch multiple tickers in single flow run
- Use Prefect's parallel task execution for independent tasks

## Development

### Run Tests During Development

```bash
# Watch mode (reruns tests on file changes)
uv run --with pytest python -m pytest tests/ -v --tb=short --looponfail

# Verbose output with tracebacks
uv run --with pytest python -m pytest tests/ -vv --tb=long

# Run specific test
uv run --with pytest python -m pytest tests/test_sec_scraper.py::TestCIKExtraction -v
```

### Code Quality

```bash
# Format code
uv run python -m black src/ tests/

# Check types
uv run python -m mypy src/

# Lint
uv run python -m ruff check src/
```

## License

MIT

## Documentation

Complete documentation is available in the following guides:

| Document | Purpose |
|----------|---------|
| [DOCKER.md](DOCKER.md) | Docker containerization, cloud deployment (AWS, K8s, etc.) |
| [docs/TESTING.md](docs/TESTING.md) | Comprehensive testing guide (unit, integration, Docker) |
| [docs/DASK_COILED_RECOMMENDATIONS.md](docs/DASK_COILED_RECOMMENDATIONS.md) | Strategic guide on scaling with Dask/Coiled |
| [docs/DASK_COILED_QUICK_REFERENCE.md](docs/DASK_COILED_QUICK_REFERENCE.md) | Quick reference for Dask/Coiled decisions |
| [DEPLOYMENT_CHECKLIST.md](DEPLOYMENT_CHECKLIST.md) | Pre-deployment verification checklist |

## Scaling & Performance

### Current Performance

- Single ticker: ~18 seconds
- Three tickers: ~25 seconds  
- 1000 companies (sequential): ~16+ minutes
- Bottleneck: SEC API rate limit (1 request/second)

### Scaling Options

For larger workloads, see [docs/DASK_COILED_RECOMMENDATIONS.md](docs/DASK_COILED_RECOMMENDATIONS.md):

- **Phase 1 (Recommended)**: Optimization + Alpha Vantage upgrade = 3-5x improvement ($30/month)
- **Phase 2 (If needed)**: Local Dask cluster = 4-10x improvement ($50-200/month infrastructure)
- **Phase 3 (Enterprise)**: Coiled managed service = 10-50x improvement ($300-400/month)

**Current recommendation**: Implement Phase 1 optimization before considering distributed computing.

## Support

For issues or questions:
1. Check the [Testing Guide](docs/TESTING.md)
2. Review [Deployment Checklist](DEPLOYMENT_CHECKLIST.md)
3. Read [Dask/Coiled Quick Reference](docs/DASK_COILED_QUICK_REFERENCE.md) for scaling questions
4. Review [Development Instructions](.github/copilot-instructions.md)
5. Check Prefect logs for detailed error messages
